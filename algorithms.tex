\section{Coreset Algorithms}
\label{sec:algorithms}

Though the algorithms vary in details, coreset constructions come in one of the following two flavours:

\begin{enumerate}
\item {\bf Movement-based constructions:} Such algorithms compute a coreset $\Omega$ with $T$ points given some input point set $A$ such that $\cost_{\Omega}(C)\ll \opt$, where $\opt$ is the cost of an optimal $k$-means clustering of $A$. 
The coreset guarantee then follows as a consequence of the triangle inequality. These algorithms all have an exponential dependency on the dimension $d$, and therefore have been overtaken by sampling-based methods. Nevertheless, these constructions are more robust to various constrained clustering formulations~\cite{HuangJV19,SSS19} and continue to be popular. Examples from theory include~\cite{FrahlS2005,HaM04}. 

\item {\bf Importance sampling:} Points are sampled proportionate to their impact on the cost of any given candidate solution. The idealized distribution samples proportionate to the sensitivity which for a point $p$ is defined as $sens(p):=\sup_{C} \frac{\min_{c\in C} \dist^2(p,c)}{\cost_A(C)}$ and weighted by their inverse sampling probability. The sensitivities are hard to compute exactly but much work exists on how to find other distributions with very similar properties. In terms of theoretical performance, sensitivity sampling has largely replaced movement-based constructions, see for example~\cite{FeldmanL11,LangbergS10}.  
\end{enumerate}

Of course, there exist algorithms that draw on techniques from both, see for example~\cite{Cohen-AddadSS21}. In what follows, we will survey implementations of various coreset constructions that we will evaluate later.


{\bf StreamKM++~\cite{AckermannMRSLS12}:} The popular $k$-means++ algorithm~\cite{ArV07} computes a set of centers $K$ by iteratively sampling a point $p$ in $A$ proportionate to $\min_{q\in K} \dist^2(p,q)$ and adding it to $K$. The procedure terminates once the desired number of centers has been reached. The first center is typically picked uniformly at random.
The StreamKM++ paper runs the $k$-means++ algorithms for $T$ iterations, where $T$ is the desired coreset size. At the end, every point $q$ in $K$ is weighted by the number of points in $A$ closest to it. While the construction has elements of importance sampling, the analysis is largely movement-based. The provable bound required for the algorithm to compute a coreset is $O\left(\frac{k\log n}{\delta^{d/2}\varepsilon^d}\cdot \log^{d/2} \frac{k\log n}{\delta^{d/2}\varepsilon^d}\right)$. Despite its simplicity, its running time compares unfavourably to all other constructions.

{\bf BICO~\cite{FGSSS13}:} BICO combines the very fast, but poor quality clustering algorithm BIRCH~\cite{ZRL97} with the movement-based analysis from~\cite{FrahlS2005,HaM04}. The clustering is organized by way of a hierarchical decomposition: When adding a point $p$ to one of the coreset points $\Omega$ at level $i$, it first finds the closest point $q$ in $\Omega$. If $p$ is too far away from $q$, a new cluster is opened with center at $p$. Otherwise $p$ is either added to the same cluster as $q$, or, if adding $p$ to $q$'s cluster increases the clustering cost beyond a certain threshold, the algorithm attempts to add $p$ to the child-clusters of $q$. The procedure then continues recursively. The provable bound required for the algorithm to compute a coreset is $O\left(k\varepsilon^{-d-2}\log n\right)$.

{\bf Ray Maker~\cite{HaK07}:} The algorithm computes an initial solution with $k$ centers which is a constant factor approximation of the optimal clustering. Around each center, $O(1/\epsilon^{d-1})$ random rays are created which span the hyperplane. Next, each point $p \in A$ is snapped to its closest ray resulting in a set of one-dimensional points associated with each ray. Afterwards, a coreset is created for each ray by computing an optimal 1D clustering with $k^2/\epsilon^2$ centers and weighing each center by the number of points in each cluster. The final coreset is composed of the coresets computed for all the rays.
The provable bound required for the algorithm to compute a coreset is $O(k^3 \cdot \varepsilon^{-d-1})$. The algorithm has recently received some attention due to its applicability to the fair clustering problem~\cite{HuangJV19}.


{\bf Sensitivity Sampling~\cite{FeldmanL11}:} The simplest implementation of sensitivity sampling first computes an $(O(1),O(1))$ bicriteria approximation\footnote{An $(\alpha,\beta)$ bicriteria approximation computes an $\alpha$ approximation using $\beta\cdot k$ many centers.}, for example by running $k$-means++ for $2k$ iterations~\cite{Wei16}. Let $K$ be the $2k$ clustering thus computed and let $K_i$ be an arbitrary cluster of $K$ with center $q_i$. Subsequently, the algorithm picks points proportionate to $\frac{\dist^2(p,q)}{\cost_{K_i}(\{q_i\})} + \frac{1}{|K_i|}$ and weighs any point by its inverse sampling probability. Let $|\hat{K_i}|$ be the estimated number of points in the sample. Finally, the algorithm weighs each $q_i$ by $(1+\eps)\cdot |K_i| - |\hat{K_i}|$. The provable bound required for the algorithm to compute a coreset is $\tilde O\left(kd\varepsilon^{-4}\right)$ (\cite{FeldmanL11}),
$\tilde O\left(k\varepsilon^{-6}\right)$ (\cite{huang2020coresets}), or $\tilde O\left(k^2\varepsilon^{-4}\right)$ (\cite{BravermanJKW21}).

{\bf Group Sampling~\cite{Cohen-AddadSS21}:} First, the algorithm computes an $O(1)$ approximation (or a bicriteria approximation) $K$. Subsequently, the algorithm preprocesses the input into groups such that (1) for any two points $p,p'\in K_i$, their cost is identical up to constant factors and (2) for any two clusters $K_i,K_j$, their cost is identical up to constant factors. In every group, Group Sampling now samples points proportionate to their cost. The authors of~\cite{Cohen-AddadSS21} show that there always exist a partitioning into $\log^2 1/\varepsilon$ groups. Points not contained in a group are snapped to their closest center $q$ in $K$. $q$ is weighted by the number of points snapped to it. The provable bound required for the algorithm to compute a coreset is $\tilde O\left(k\varepsilon^{-2}\min(d,k,\varepsilon^{-2})\right)$ (\cite{CLSS22}). While this improves over sensitivity sampling, it is generally slower and not as easy to implement.

Finally, we note that some of the more popular algorithms in theory have not been mentioned here. For example, Chen's \cite{Chen09} construction is particularly popular among theoreticians. The Group Sampling algorithm by \cite{Cohen-AddadSS21} is an extension and improvement of Chen's method. Thus, the performance of Group Sampling is also indicative of Chen's algorithm.


\paragraph*{Dimension Reduction}
%\label{sec:dim_reduction}
Finally, we also combine coreset constructions with a variety of dimension reduction techniques. Starting with~\cite{DrineasFKVV04}, a series of results \cite{BecchettiBC0S19,BoutsidisMD09,BoutsidisZD10,BoutsidisZMD15,CEMMP15,Cohen-AddadS17,FeldmanSS20,FKW19,KuK10,MakarychevMR19,SohlerW18} explored the possibility of using dimension reduction methods for $k$-clustering, with a particular focus on principal component analysis (PCA) and random projections. The seminal paper by Feldman, Schmidt, and Sohler~\cite{FeldmanSS20} was the first to use dimension reduction to obtain smaller coresets for $k$-means. Movement-based coresets in particular often have an exponential dependency on the dimension, which can be alleviated with some form of dimension reduction, both in theory~\cite{SSS19} and in practice~\cite{KappmeierS015}.
There are essentially two main dimension reduction techniques for coresets.

{\bf Principal Component Analysis:} Feldman, Schmidt, and Sohler~\cite{FeldmanSS20} showed that projecting an input $A$ onto the first $O(k/\varepsilon^2)$ principal components is a coreset. This coreset still consists of $n$ points, but they now lie in low dimension. The analysis was subsequently tightened by~\cite{CEMMP15} and extended to other center-based cost functions by~\cite{SohlerW18}. Although its target dimension is generally worse than those based on random projections and terminal embeddings, there is nevertheless reasons for using PCA regardless: It removes noise and thus may make it easier to compute a high quality coreset. For more applications of PCA to $k$-means clustering, we refer to

{\bf Terminal Embeddings:} Given a set of points $A$ in $\mathbb{R}^D$, a terminal embedding $f:\mathbb{R}^D\rightarrow \mathbb{R}^d$ preserves the pairwise distance between any point $p\in A$ and any point $q\in \mathbb{R}^D$ up to a $(1\pm \varepsilon)$ factor. The statement is related to the famous Johnson-Lindenstrauss lemma but it is stronger as it does not apply to only the pairwise distances of $A$. Nevertheless, the same target dimension is sufficient. Terminal embeddings were studied by~\cite{CherapanamjeriN21,ElkinFN17,MahabadiMMR18,NaN18}, with Narayanan and Nelson \cite{NaN18} achieving an optimal target dimension of $O(\varepsilon^{-2}\log n)$, where $n$ is the number of points. We note that terminal embeddings, combined with an iterative application of the coreset construction from \cite{BravermanJKW21}, can reduce the target dimension to a factor $\tilde{O}(\varepsilon^{-2} \log k)$. This is mainly of theoretical interest, as in practice the deciding factor wrt the target dimension is the precision, rather than dependencies on $\log n$ and $\log k$. For applications to coresets, we refer to \cite{BecchettiBC0S19,Cohen-AddadSS21,huang2020coresets}. For an empirical evaluation of random projections, which form the basis of all known terminal embeddings, we refer to Venkatsubramanian and Wang~\cite{VenkatasubramanianW11}.


